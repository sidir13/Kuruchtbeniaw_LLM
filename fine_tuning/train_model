import os
import json

from checkpoint import find_lastest_checkpoint

from datasets import load_dataset, load_from_disk
from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling, Trainer, TrainingArguments, trainer_callback
from transformers.integrations import MLflowCallback
import torch
import mlflow
import mlflow.pytorch


mlflow.set_experiment('gpt-neo-125M')
mlflow.set_tracking_uri(uri="http://127.0.0.1:5000/")


# Load dataset
dataset = load_dataset('json', data_files={
    'train': './data/data_splits/train_small.jsonl',
    'validation': './data/data_splits/val_small.jsonl'
})


# Load model and tokenizer
tokenizer = AutoTokenizer.from_pretrained("./saved_model/gpt_neo")
model = AutoModelForCausalLM.from_pretrained("./saved_model/gpt_neo")


if tokenizer.pad_token is None:
    tokenizer.add_special_tokens({'pad_token': '[PAD]'})
    model.resize_token_embeddings(len(tokenizer))


# Tokenization
def tokenize_function(examples):
    return tokenizer(examples["text"],
                     padding="max_length",
                     truncation=True,
                     max_length=512)

# Set at True if you are fine-tuning from zero and not a checkpoint
if False:
    tokenized_dataset = dataset.map(tokenize_function, batched=True)
    tokenized_dataset.save_to_disk("./data/data_processing/tokenized_dataset")
else:
    tokenized_dataset = load_from_disk(
        "./data/data_processing/tokenized_dataset")


lr = 1e-4
num_train_epochs = 1
batch_size = 8
w_d = 0.01
n_train_e = 1
output_d = "./saved_model/gpt_neo_finetuned"

training_args = TrainingArguments(
    output_dir=output_d,
    save_strategy="steps",
    save_total_limit=2,
    save_steps=5,
    learning_rate=lr,
    weight_decay=w_d,
    num_train_epochs=n_train_e,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    logging_dir="./saved_model/gpt_neo_finetuned/logs",
    fp16=torch.cuda.is_available(),
    report_to=[]
)


# Data collator
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["validation"],
    tokenizer=tokenizer,
    data_collator=data_collator
)


with mlflow.start_run():
    # Log training hyperparameters
    mlflow.log_params({
        "learning_rate": lr,
        "weight_decay": w_d,
        "num_train_epochs": n_train_e,
        "batch_size": batch_size
    })
    print("\n")
    print("Training begin...")
    print("\n")
    # Train the model
    if False: # os.path.exists(output_d) and len(os.listdir(output_d)) > 0:
        print("Checkpoint found")
        lastest_checkpoint = find_lastest_checkpoint(output_d)
        trainer.train(resume_from_checkpoint=output_d +
                      "/" + lastest_checkpoint)
    else:
        trainer.train()

    # Log the model in MLflow
    mlflow.pytorch.log_model(model, "gpt_neo_model")

    # Save model to disk
    model_save_path = "./model_saved/gpt_neo_finetuned"
    model.save_pretrained(model_save_path)
    tokenizer.save_pretrained(model_save_path)


print("All done")
